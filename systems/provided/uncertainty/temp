
def get_support_data(instruments, dbtype=None, filename=None, multiplefiles=True):
    """
    From diags; quicker

    """
    adj_vols={}
    fxs={}
    adj_prices={}
    norm_returns={}
    for code in instruments:
        (adj_vol, norm_return, fx, adj_price)=get_support(code, dbtype, filename, multiplefiles)
        adj_vols[code]=adj_vol
        norm_returns[code]=norm_return
        fxs[code]=fx
        adj_prices[code]=adj_price


    return (adj_vols, norm_returns, fxs, adj_prices)

def get_support(code, dbtype=None, filename=None, multiplefiles=True):
    adj_vol=diagnostic(dbtype=dbtype, filename=filename, code=code, system="rawdata", system2="adj_vol", multiplefiles=multiplefiles).read()
    norm_return=diagnostic(dbtype=dbtype, filename=filename, code=code, system="rawdata", system2="capped_norm_return", multiplefiles=multiplefiles).read()
    fx=diagnostic(dbtype=dbtype, filename=filename, code=code, system="rawdata", system2="fx", multiplefiles=multiplefiles).read()
    adj_price=diagnostic(dbtype=dbtype, filename=filename, code=code, system="rawdata", system2="adj_prices", multiplefiles=multiplefiles).read()

    return (adj_vol, norm_return, fx, adj_price)


def live_cross_vol_calc_data(dbtype, combineconfig):
    cvoldata=blob()

    instruments=combineconfig.instruments


    (adj_vols, norm_returns, fxs, adj_prices)=get_support_data(instruments, dbtype=dbtype, multiplefiles=True)


    cvoldata.u(adj_vols=adj_vols, norm_returns=norm_returns, fxs=fxs, adj_prices=adj_prices)

    ## get last signals

    signals_vector=[diagnostic(dbtype, code=code, system="combiner", system2="signals_with_weights").read() for
                    code in instruments]

    signals_mat=pd.concat(signals_vector, axis=1)
    signals_mat.columns=instruments

    cvoldata.u(signals_mat=signals_mat)

    return cvoldata

def live_cross_vol_config():
    cvolconfig=blob()
    sigbudget=3.0
    maxrisknorm=2.0
    maxriskhighvol=3.0

    ## Data sets
    ##(data_start, data_end, apply_start_date, apply_end_date)
    date_vector=[(datetime.datetime.now()+pd.DateOffset(years=-5),datetime.datetime.now(),
                  datetime.datetime.now() - pd.DateOffset(days=10), datetime.datetime.now())]

    qpoint=0.9
    maxmultiplier=5.0

    config=get_config()
    instruments=list(config.Instrument)


    cvolconfig.u(sigbudget=sigbudget, maxrisknorm=maxrisknorm, maxriskhighvol=maxriskhighvol,
                 date_vector=date_vector, qpoint=qpoint, maxmultiplier=maxmultiplier,
                 instruments=instruments)

    return cvolconfig

def cross_vol_calc(dbtype):
    """
    This LIVE function, called periodically, calculates the cross sectional risk parameters for one period
        writes diags.
    """

    log=logger()
    diag=diagnostic(dbtype,  system="combiner", forcedb=True)

    log.info("Getting support data")

    cvolconfig=live_cross_vol_config()

    crossvoldata=live_cross_vol_calc_data(dbtype, cvolconfig)



    risk_scalar=cross_vol_calc_func(diag, log, crossvoldata, cvolconfig)

    return risk_scalar

def get_vol_points(adj_vols, signals_mat, qpoint=0.9, maxmultiplier=5.0):
    """
    Returns ratio of vol now to 90% quantile vol; 1.0 means its the same, 2.0 means 90% vol is doubled

    Any value below 1.0 (i.e. vol is at 90% plus now) is winsorised

    returned as a dict
    """


    ans={}

    for icode in adj_vols.keys():
        y=adj_vols[icode]
        x=signals_mat[icode]
        vpi=[]
        for didx in y.index:
            ysub=y[didx-pd.DateOffset(years=5):didx]
            vpi.append(min(max(1.0, ysub.quantile(qpoint)/ysub[-1]), maxmultiplier))
        vpi=pd.TimeSeries(vpi, y.index)
        vpi=daily_resample(vpi, x).ffill()
        ans[icode]=vpi

    return ans

    risk_scalar=cross_vol_calc_func(diag, log, crossvoldata, cvolconfig)


def cross_vol_calc_func(diag, log, data, config):
    """
    Run the cross vol calc for N periods as defined in config
    """


date_vector=config.date_vector

totalsignal=[]
risk_scalar_totsig=[]
natural_risk=[]
risk_scalar_natrisk=[]
higher_risk_signals=[]
risk_scalar_highrisk=[]
risk_scalar=[]
higher_risk=[]
effective_risk=[]

log.info("Getting vol points...")
vol_points=get_vol_points(data.adj_vols, data.signals_mat, qpoint=config.qpoint, maxmultiplier=config.maxmultiplier)
data.u(vol_points=vol_points)

for date_items in date_vector:
    log.info("Running with data from %s to %s applying to %s to %s" % tuple([str(x) for x in date_items]))
    ## call the function, which returns scalars or lists (CHECK)
    (totalsignal_i, risk_scalar_totsig_i, natural_risk_i, risk_scalar_natrisk_i,higher_risk_signals_i, risk_scalar_highrisk_i, risk_scalar_i, higher_risk_i,    effective_risk_i, rmat)=cross_vol_calc_oneperiod(date_items, data, config)
    totalsignal.append(totalsignal_i)
    risk_scalar_totsig.append(risk_scalar_totsig_i)
    natural_risk.append(natural_risk_i)
    risk_scalar_natrisk.append(risk_scalar_natrisk_i)
    higher_risk_signals.append(higher_risk_signals_i)
    risk_scalar_highrisk.append(risk_scalar_highrisk_i)
    risk_scalar.append(risk_scalar_i)
    higher_risk.append(higher_risk_i)
    effective_risk.append(effective_risk_i)

## Concat the time series above

totalsignal=pd.concat(totalsignal, axis=0)
risk_scalar_totsig=pd.concat(risk_scalar_totsig, axis=0)
natural_risk=pd.concat(natural_risk, axis=0)
risk_scalar_natrisk=pd.concat(risk_scalar_natrisk, axis=0)
risk_scalar_highrisk=pd.concat(risk_scalar_highrisk, axis=0)
risk_scalar=pd.concat(risk_scalar, axis=0)
higher_risk=pd.concat(higher_risk, axis=0)
effective_risk=pd.concat(effective_risk, axis=0)
higher_risk_signals=pd.concat(higher_risk_signals, axis=0)


    ## write some diags


    diag.w(create_dull_ts(config.sigbudget, totalsignal.index), system2="sigbudget")
    diag.w(create_dull_ts(config.maxrisknorm, totalsignal.index), system2="maxrisknorm")
    diag.w(create_dull_ts(config.maxriskhighvol, totalsignal.index), system2="maxriskhighvol")

    ## The above will return pandas ts which could have one row

    log.info("Risk scaling")

    diag.w(higher_risk, system2="higher_risk")
    diag.w(risk_scalar_natrisk, system2="risk_scalar_natrisk")
    diag.w(natural_risk, system2="natural_risk")

    diag.w(risk_scalar_highrisk, system2="risk_scalar_highrisk")
    diag.w(effective_risk, system2="effective_risk")

    diag.w(risk_scalar_totsig, system2="risk_scalar_totsig")
    diag.w(totalsignal, system2="totalsignal")

    diag.w(risk_scalar, system2="risk_scalar")

    ## Following should be dataframes; need to write timeseries which will become scalars in db force mode
    [diag.w(higher_risk_signals[i], code=config.instruments[i], system2="higher_risk_signals") for i in range(len(config.instruments))]
    [diag.w(vol_points[instrument], code=instrument, system2="volbump") for instrument in config.instruments]

    return risk_scalar





def cross_vol_calc_oneperiod(date_items, data, config):
    """
    Single period vol calc

    adj_returns, adj_vols should be up to the date snapshot

    Returns tuple (totalsignal, risk_scalar_totsig, natural_risk, risk_scalar_natrisk,    higher_risk_signals, risk_scalar_highrisk, risk_scalar, higher_risk)
    """

    signals_mat=data.signals_mat
    instruments=config.instruments
    norm_returns=data.norm_returns
    vol_points=data.vol_points
    sigbudget=config.sigbudget
    maxrisknorm=config.maxrisknorm
    maxriskhighvol=config.maxriskhighvol

    (data_start, data_end, apply_start_date, apply_end_date)=date_items

    ##Cut data
    #signals_mat=signals_mat[apply_start_date:apply_end_date]


    ## max signal budget - don't worry about correlations
    ## Needs to be timeseries
    signals_mat_f=signals_mat.ffill()

    signals_mat_f[np.isnan(signals_mat_f)]=0.0
    totalsignal=signals_mat_f.abs().sum(axis=1)

    ## Needs to be timeseries
    risk_scalar_totsig=pd.TimeSeries([min(1.0, sigbudget/tsi) for tsi in totalsignal.values], totalsignal.index)

    ## risk with normal vols and correlations

    ## note using data dates to avoid in sampleness


    rmat=[norm_returns[icode][data_start:data_end] for icode in instruments]

    def _fill_it_in(x, data_start, data_end):
        ## replace empty with double nan
        if len(x.index)==0:
            return pd.TimeSeries([np.nan]*2, [data_start,data_end])
        else:
            return x

    rmat=[_fill_it_in(x, data_start, data_end) for x in rmat]
    rmat=pd.concat(rmat, axis=1)
    rmat.columns=instruments


    raw_corr=correlation_matrix(rmat)
    raw_corr[np.isnan(raw_corr)]=0.0

    def _riskrow(signals_mat, rmat, idx):
        return (np.array(signals_mat.values[idx]).T.dot(raw_corr).dot(signals_mat.values[idx]))**.5

    natural_risk=[_riskrow(signals_mat_f, rmat, idx) for idx in range(signals_mat_f.shape[0])]
    natural_risk=pd.TimeSeries(natural_risk, index=signals_mat_f.index)

    risk_scalar_natrisk=pd.TimeSeries([min(1.0, maxrisknorm/nri) for nri in natural_risk.values], natural_risk.index)

    ## risk with normal correlation and 90% quantile vols (deals with very low vols)
    ## Returns a data frame

    def _volrow(signals_mat_f, iname, vpoints, didx):

        return signals_mat_f[iname][didx]*vpoints[iname][:didx][-1]

    def _volcol(signals_mat_f, iname, vpoints):
        return pd.TimeSeries([_volrow(signals_mat_f, iname, vpoints, didx) for didx in signals_mat_f[iname].index])

    higher_risk_signals=pd.concat([_volcol(signals_mat_f, iname, vol_points) for iname in signals_mat.columns], axis=1)
    higher_risk_signals[np.isnan(higher_risk_signals)]=0.0
    higher_risk=[_riskrow(higher_risk_signals, rmat, idx) for idx in range(higher_risk_signals.shape[0])]
    higher_risk=pd.TimeSeries(higher_risk, signals_mat.index)

    risk_scalar_highrisk=pd.TimeSeries([min(1.0, maxriskhighvol/hri) for hri in higher_risk], higher_risk.index)

    risk_scalar=[min(risk_scalar_highrisk.values[i], risk_scalar_natrisk.values[i], risk_scalar_totsig.values[i]) for i in range(len(signals_mat.index))]
    risk_scalar=pd.TimeSeries(risk_scalar, signals_mat.index)

    effective_risk=pd.TimeSeries([natural_risk.values[i]*risk_scalar.values[i] for i in range(len(risk_scalar.index))], signals_mat.index)

    ## time series,  timeseries,             timeseries,    timeseries
    ## dataframe,         timeseries,            timeseries,  timeseries,  timeseries,  matrix

    return (totalsignal, risk_scalar_totsig, natural_risk, risk_scalar_natrisk,    higher_risk_signals, risk_scalar_highrisk, risk_scalar, higher_risk, effective_risk, rmat)


def get_signal_data_for_code(code, dbtype=None, filepattern=None, system="make_signals", system2="final_signal", multiplefiles=True):
    """
    Returns signal value for a market code
    """

    sp=diagnostic(dbtype=dbtype, filepattern=filepattern, code=code, system=system, system2=system2, multiplefiles=multiplefiles).read()

    return sp




def scaling_data(code, dbtype=None, filepattern=None):
    scaledata=blob()

    risk_scalar=diagnostic(dbtype=dbtype, filepattern=filepattern,  system="combiner", system2="risk_scalar").read()
    signal=get_signal_data_for_code(code, dbtype=dbtype, filepattern=filepattern, system="combiner", system2="signals_with_weights")

    scaledata.u(risk_scalar=risk_scalar, signal=signal)

    return scaledata




def live_combiner_config(code):
    combineconfig=blob()

    configfile=get_config()
    weight=configfile[configfile.Instrument==code].instrumentWeight.irow(-1)
    combineconfig.u(weight=weight)

    applypi="function_scalar(pi, signal)"
    weightfunctioncall="function_scalar(weight, signal)"

    functions=dict(applypi=applypi, weightfunctioncall=weightfunctioncall)
    combineconfig.u(functions=functions)

    pi=2.5
    combineconfig.u(pi=pi)

    return combineconfig


def function_scalar(x, signal):
    return x*signal

def timeseries_scalar(x, signal):

    if type(x) is list:
        if len(x)==1:
            return timeseries_scalar(x[0], signal)
        ## recursive
        return timeseries_scalar(x[1:], timeseries_scalar(x[0], signal))

    xx=x.sort_index()
    new_s=signal.sort_index()
    xx=uniquets(xx)
    xx=xx.reindex(new_s.index, method='ffill')
    ans=[xx.values[idx]*new_s.values[idx] for idx in range(len(new_s.index))]
    ans=pd.TimeSeries(ans, new_s.index)

    return ans


def combiner_data( code, dbtype=None, filepattern=None):
    combinerdata=blob()

    ## Need to change to time series
    signal=get_signal_data_for_code(code, dbtype, filepattern)
    combinerdata.u(signal_input=signal)

    return combinerdata

def combiner(dbtype, code, applyscalar=True):
    """
    LIVE combining function
    """
    log=logger()
    ## Need to force so writes one value at a time doesn't use files
    diag=diagnostic(dbtype,  code=code, system="combiner", forcedb=True)

    config_blob=live_combiner_config(code)
    data_blob=combiner_data(code, dbtype=dbtype)

    log.info("Weight for %s is %f and pi is %f" % (code, config_blob.weight, config_blob.pi))
    signal=combiner_func(config_blob, data_blob, diag, log)

    if applyscalar:
        scaledata=scaling_data(code, dbtype=dbtype)
        signal=apply_scalar_func(config_blob, scaledata, diag, log)

    log.info("Signal is %f" % signal.values[-1])
    return signal

def combine_all_markets(dbtype, applyscalar=True):
    ## convenience function
    config=get_config()
    instruments=list(config.Instrument)

    for code in instruments:
        combiner(dbtype, code, applyscalar)

def combiner_func(config_blob, data_blob, diag, log):
    """
    Needs to be able to handle pandas time series, which will be length one for live
    """

    signal=data_blob.signal_input

    weight=config_blob.weight
    diag.w(weight , system2="weights")
    diag.w(signal, system2="signal_start")

    log.info("Applying pi 'n' weights")
    pi=config_blob.pi
    diag.w(pi, system2="pi")

    signal=eval(config_blob.functions['applypi'])

    diag.w(signal, system2="signals_with_pi")

    signal=eval(config_blob.functions['weightfunctioncall'])

    diag.w(signal , system2="signals_with_weights")

    return signal



def apply_scalar_func(config_blob, scalingdata, diag, log):

    signal=scalingdata.signal.ffill()
    risk_scalar=scalingdata.risk_scalar

    log.info("Writing final signals")
    signal=timeseries_scalar(risk_scalar, signal)
    diag.w(signal , system2="signals_with_risk_scalar")

    diag.w(signal , system2="signals_final")

    log.info("And done")

    return signal
